{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet 1000-Class Training with ResNet50\n",
    "## Target: 82%+ Validation Accuracy\n",
    "\n",
    "This notebook trains ResNet50 on **full ImageNet-1K** (1000 classes) with:\n",
    "- **~1.28M training images** (~1,300 per class)\n",
    "- **~50,000 validation images** (~50 per class)\n",
    "- **Medium data augmentation** for stable training\n",
    "- **Training from scratch** using LR finder + OneCycleLR\n",
    "- **Batch size 256** optimized for AWS GPU instances\n",
    "\n",
    "**Recommended Hardware:**\n",
    "- AWS g5.xlarge: 1x NVIDIA A10G (24GB VRAM) - $1.006/hour\n",
    "- AWS g5.2xlarge: 1x NVIDIA A10G (24GB VRAM) + more CPU - $1.212/hour  \n",
    "- AWS p3.2xlarge: 1x NVIDIA V100 (16GB VRAM) - $3.06/hour (faster)\n",
    "\n",
    "**Expected Training Time:**\n",
    "- g5.xlarge: ~8-10 min/epoch × 90 epochs = ~12-15 hours\n",
    "- p3.2xlarge: ~5-6 min/epoch × 90 epochs = ~7-9 hours\n",
    "\n",
    "**Expected Cost:**\n",
    "- g5.xlarge: ~$12-15 (on-demand) or ~$4-5 (spot)\n",
    "- p3.2xlarge: ~$21-27 (on-demand) or ~$6-8 (spot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Local imports\n",
    "from model import create_resnet50, get_model_stats\n",
    "from data_loader_full import get_full_dataloaders\n",
    "from train import Trainer\n",
    "\n",
    "print(\"✓ Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Metal Performance Shaders (MPS)\")\n",
    "    print(\"⚠️  Warning: MPS training will be very slow for 1000 classes!\")\n",
    "    print(\"⚠️  Recommend using AWS GPU instance instead.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"⚠️  Using CPU (NOT RECOMMENDED for 1000-class training!)\")\n",
    "    print(\"⚠️  This will take days/weeks. Use AWS GPU instance.\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Training Configuration\n",
    "\n",
    "**Optimized for 82%+ validation accuracy on 1000 classes (training from scratch)**\n",
    "\n",
    "**Key differences from 100-class training:**\n",
    "- More classes (1000 vs 100) → harder task\n",
    "- More data (1.28M vs 130k) → longer training\n",
    "- Larger batch size (256 vs 128) → better for AWS GPUs\n",
    "- Medium augmentation (not heavy) → more stable with larger dataset\n",
    "- More epochs may be needed (90-120) for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data Configuration\n",
    "    'data_dir': './imagenet_1000class_data',  # CHANGED: Full ImageNet path\n",
    "    'num_classes': 1000,  # CHANGED: Full 1000 classes\n",
    "    \n",
    "    # Training Configuration\n",
    "    'num_epochs': 90,  # 90-120 epochs for 82%+ accuracy from scratch\n",
    "    'batch_size': 256,  # CHANGED: Larger batch for AWS GPU (has more memory)\n",
    "    'num_workers': 8,  # CHANGED: AWS instances have more CPU cores\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # Augmentation (CHANGED: medium for stability with 1000 classes)\n",
    "    'augmentation_strength': 'medium',  # Medium augmentation for 1.28M samples\n",
    "    \n",
    "    # Learning Rate Configuration\n",
    "    'find_lr': True,  # Auto-find optimal learning rate\n",
    "    'initial_lr': 0.05,  # Starting LR (will be overridden by LR finder)\n",
    "    'max_lr': 0.3,      # Max LR (will be overridden by LR finder)\n",
    "    'lr_finder_iterations': 200,  # Iterations for LR finder\n",
    "    \n",
    "    # Regularization\n",
    "    'weight_decay': 1e-4,  # L2 regularization\n",
    "    'label_smoothing': 0.1,  # Label smoothing (0.1 is standard for ImageNet)\n",
    "    'max_grad_norm': 1.0,  # Gradient clipping\n",
    "    \n",
    "    # Model Configuration\n",
    "    'zero_init_residual': True,  # Zero-init residual connections\n",
    "    \n",
    "    # OneCycleLR Configuration (based on 100-class learnings)\n",
    "    'pct_start': 0.3,  # 30% warmup (can adjust to 0.5-0.6 for longer warmup)\n",
    "    'div_factor': 25.0,  # initial_lr = max_lr / 25\n",
    "    'final_div_factor': 1e4,  # final_lr = initial_lr / 10000\n",
    "    \n",
    "    # Checkpoint Configuration\n",
    "    'checkpoint_dir': './checkpoints_1000class',\n",
    "    'save_frequency': 5,  # Save every 5 epochs\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION - ImageNet 1000 Classes\")\n",
    "print(\"=\"*70)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:30s}: {value}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n⚠️  IMPORTANT: This will train from scratch on 1.28M images!\")\n",
    "print(\"Expected time: 12-15 hours (g5.xlarge) or 7-9 hours (p3.2xlarge)\")\n",
    "print(\"Expected cost: $12-15 (g5.xlarge) or $21-27 (p3.2xlarge)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Full ImageNet-1K Dataset\n",
    "\n",
    "**⚠️  DATASET DOWNLOAD REQUIRED ⚠️**\n",
    "\n",
    "This notebook assumes the full ImageNet-1K dataset is available at `./imagenet_1000class_data/`\n",
    "\n",
    "**Option 1: Download using Hugging Face (RECOMMENDED for AWS)**\n",
    "```python\n",
    "# Run this ONCE on AWS to download full dataset (~150 GB, 2-3 hours):\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=False)\n",
    "dataset = load_dataset(\"ILSVRC/imagenet-1k\", split=\"validation\", streaming=False)\n",
    "# Follow data_loader_full.py documentation for complete setup\n",
    "```\n",
    "\n",
    "**Option 2: Download from Official ImageNet**\n",
    "- Register at https://image-net.org/\n",
    "- Download ILSVRC2012 training and validation sets\n",
    "- Extract to `./imagenet_1000class_data/train/` and `./imagenet_1000class_data/val/`\n",
    "\n",
    "**Expected Structure:**\n",
    "```\n",
    "imagenet_1000class_data/\n",
    "├── train/\n",
    "│   ├── n01440764/  (1,300 images)\n",
    "│   ├── n01443537/  (1,300 images)\n",
    "│   └── ... (1000 classes total)\n",
    "└── val/\n",
    "    ├── n01440764/  (50 images)\n",
    "    ├── n01443537/  (50 images)\n",
    "    └── ... (1000 classes total)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"LOADING FULL IMAGENET-1K DATASET (1000 CLASSES)\")\nprint(\"=\"*70)\nprint(\"Expected dataset size:\")\nprint(\"  Training:   ~1,281,167 images (~1,281 per class)\")\nprint(\"  Validation: ~50,000 images (~50 per class)\")\nprint(\"  Total disk space: ~140-150 GB\")\nprint(\"  Number of classes: 1000\")\nprint(\"=\"*70 + \"\\n\")\n\ntry:\n    train_loader, val_loader, num_classes, class_names = get_full_dataloaders(\n        data_dir=config['data_dir'],\n        batch_size=config['batch_size'],\n        num_workers=config['num_workers'],\n        advanced_augmentation=True,\n        augmentation_strength=config['augmentation_strength'],\n        pin_memory=config['pin_memory'],\n        distributed=False,  # Set to True for multi-GPU training\n        auto_download=True  # Auto-download if dataset not found (WARNING: ~150GB!)\n    )\n\n    print(f\"\\n✓ Full ImageNet data loaded successfully!\")\n    print(f\"  Number of classes: {num_classes}\")\n    print(f\"  Training batches: {len(train_loader):,}\")\n    print(f\"  Validation batches: {len(val_loader):,}\")\n    print(f\"  Augmentation strength: {config['augmentation_strength']}\")\n    print(f\"\\n  With batch size {config['batch_size']}:\")\n    print(f\"    ~{len(train_loader)} iterations per epoch\")\n    print(f\"    ~{len(train_loader) * config['num_epochs']:,} total iterations\")\n    \nexcept FileNotFoundError as e:\n    print(\"\\n\" + \"=\"*70)\n    print(\"❌ DATASET DOWNLOAD FAILED\")\n    print(\"=\"*70)\n    print(str(e))\n    print(\"\\n\" + \"=\"*70)\n    print(\"ALTERNATIVE METHODS TO DOWNLOAD:\")\n    print(\"=\"*70)\n    print(\"\\n1. Manually run download script:\")\n    print(\"   python data_loader_full.py --download\")\n    print(\"\\n2. Or download from official ImageNet website\")\n    print(\"\\n3. Ensure dataset is extracted to:\")\n    print(f\"   {config['data_dir']}/train/  (1000 class folders)\")\n    print(f\"   {config['data_dir']}/val/    (1000 class folders)\")\n    print(\"=\"*70)\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Visualize Sample Images\n",
    "\n",
    "Verify data loading and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_normalisation import denormalize_image\n",
    "\n",
    "# Get a batch of training images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Denormalize for visualization\n",
    "images_denorm = denormalize_image(images)\n",
    "\n",
    "# Plot first 8 images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    img = images_denorm[i].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Class: {class_names[labels[i]]}\\nLabel: {labels[i]}\", fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Sample Training Images (augmentation: {config[\"augmentation_strength\"]})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images_1000class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"Label range: [{labels.min()}, {labels.max()}]\")\n",
    "print(f\"Unique labels in batch: {len(torch.unique(labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create ResNet50 Model for 1000 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating ResNet50 model for 1000 classes (training from scratch)...\")\n",
    "\n",
    "model = create_resnet50(\n",
    "    num_classes=config['num_classes'],\n",
    "    zero_init_residual=config['zero_init_residual']\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Get model statistics\n",
    "stats = get_model_stats(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture:          ResNet50\")\n",
    "print(f\"Number of classes:     {config['num_classes']}\")\n",
    "print(f\"Total parameters:      {stats['total_parameters']:,}\")\n",
    "print(f\"Trainable parameters:  {stats['trainable_parameters']:,}\")\n",
    "print(f\"Model size:            {stats['model_size_mb']:.2f} MB\")\n",
    "print(f\"Training from:         Scratch (random initialization)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✓ Model created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Learning Rate Finder\n",
    "\n",
    "**Automatically finds optimal learning rate range for 1000-class training**\n",
    "\n",
    "**Note:** LR finder will take ~15-20 minutes on AWS GPU\n",
    "\n",
    "**Based on 100-class learnings:**\n",
    "- Lower learning rates work better with medium/heavy augmentation\n",
    "- Expected max_lr range: 0.01 - 0.1\n",
    "- If suggested max_lr > 0.3, reduce it (too aggressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['find_lr']:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LEARNING RATE FINDER - 1000 Classes\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Running LR range test to find optimal learning rates...\")\n",
    "    print(f\"This will take ~15-20 minutes on AWS GPU.\")\n",
    "    print(f\"Testing {config['lr_finder_iterations']} learning rate values.\\n\")\n",
    "    \n",
    "    from lr_finder import LRFinder\n",
    "    \n",
    "    lr_finder = LRFinder(\n",
    "        model=model,\n",
    "        optimizer=optim.SGD(model.parameters(), lr=1e-7, momentum=0.9, weight_decay=config['weight_decay']),\n",
    "        criterion=nn.CrossEntropyLoss(label_smoothing=config['label_smoothing']),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Run LR finder\n",
    "    lrs, losses, suggested_initial_lr, suggested_max_lr = lr_finder.find(\n",
    "        train_loader,\n",
    "        init_lr=1e-8,\n",
    "        end_lr=10,\n",
    "        num_iter=config['lr_finder_iterations']\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    lr_finder.plot(lrs, losses, initial_lr=suggested_initial_lr, max_lr=suggested_max_lr)\n",
    "    plt.savefig('lr_finder_1000class.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Validate and potentially adjust suggested LRs\n",
    "    print(f\"\\n📊 LR Finder Results:\")\n",
    "    print(f\"   Suggested initial_lr: {suggested_initial_lr:.6f}\")\n",
    "    print(f\"   Suggested max_lr:     {suggested_max_lr:.6f}\")\n",
    "    \n",
    "    # Safety checks based on 100-class learnings\n",
    "    if suggested_max_lr > 0.5:\n",
    "        print(f\"\\n⚠️  WARNING: Suggested max_lr ({suggested_max_lr:.2e}) is very high!\")\n",
    "        print(f\"   Based on 100-class training, reducing to safer value...\")\n",
    "        suggested_max_lr = min(suggested_max_lr, 0.3)\n",
    "        suggested_initial_lr = suggested_max_lr / config['div_factor']\n",
    "        print(f\"   Adjusted max_lr: {suggested_max_lr:.6f}\")\n",
    "        print(f\"   Adjusted initial_lr: {suggested_initial_lr:.6f}\")\n",
    "    \n",
    "    if suggested_max_lr < 0.01:\n",
    "        print(f\"\\n⚠️  WARNING: Suggested max_lr ({suggested_max_lr:.2e}) is very low!\")\n",
    "        print(f\"   Training might be very slow. Consider checking:\")\n",
    "        print(f\"   - Dataset is loading correctly\")\n",
    "        print(f\"   - Augmentation is not too aggressive\")\n",
    "        print(f\"   Using suggested values anyway...\")\n",
    "    \n",
    "    # Update config\n",
    "    config['initial_lr'] = suggested_initial_lr\n",
    "    config['max_lr'] = suggested_max_lr\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"FINAL LEARNING RATES:\")\n",
    "    print(f\"  Initial LR: {config['initial_lr']:.6f}\")\n",
    "    print(f\"  Max LR:     {config['max_lr']:.6f}\")\n",
    "    print(f\"  Ratio:      {config['max_lr']/config['initial_lr']:.1f}x\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save LR values for reference\n",
    "    with open('lr_config_1000class.txt', 'w') as f:\n",
    "        f.write(f\"FOUND_INITIAL_LR={config['initial_lr']}\\n\")\n",
    "        f.write(f\"FOUND_MAX_LR={config['max_lr']}\\n\")\n",
    "    print(f\"\\n✓ Learning rates saved to lr_config_1000class.txt\")\n",
    "    \n",
    "    # Reload model to reset weights after LR finder\n",
    "    print(\"\\nReloading model with fresh weights...\")\n",
    "    model = create_resnet50(\n",
    "        num_classes=config['num_classes'],\n",
    "        zero_init_residual=config['zero_init_residual']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"✓ Model reloaded with random initialization\")\n",
    "    \nelse:\n",
    "    print(\"\\n⚠️  Skipping LR finder (using manual LR values)\")\n",
    "    print(f\"  Initial LR: {config['initial_lr']}\")\n",
    "    print(f\"  Max LR:     {config['max_lr']}\")\n",
    "    print(\"\\n⚠️  WARNING: For 1000-class training, LR finder is HIGHLY RECOMMENDED!\")\n",
    "    print(\"   Set 'find_lr': True in config for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Create Optimizer, Scheduler, and Trainer\n",
    "\n",
    "**Using OneCycleLR based on successful 100-class training**\n",
    "\n",
    "**Key learnings from 100-class training:**\n",
    "- Lower LR often works better than suggested max_lr\n",
    "- pct_start=0.3-0.6 depending on convergence pattern\n",
    "- SGD with Nesterov momentum is reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating optimizer and scheduler...\")\n",
    "\n",
    "# Optimizer: SGD with Nesterov momentum\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config['initial_lr'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=config['weight_decay'],\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "# Scheduler: OneCycleLR for optimal convergence\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config['max_lr'],\n",
    "    epochs=config['num_epochs'],\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=config['pct_start'],  # Warmup percentage\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=config['div_factor'],  # initial_lr = max_lr / div_factor\n",
    "    final_div_factor=config['final_div_factor']  # final_lr = initial_lr / final_div_factor\n",
    ")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    max_grad_norm=config['max_grad_norm']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Optimizer:           SGD with Nesterov momentum\")\n",
    "print(f\"Scheduler:           OneCycleLR\")\n",
    "print(f\"Initial LR:          {config['initial_lr']:.6f}\")\n",
    "print(f\"Max LR:              {config['max_lr']:.6f}\")\n",
    "print(f\"Warmup:              {config['pct_start']*100:.0f}% of epochs (first {int(config['num_epochs']*config['pct_start'])} epochs)\")\n",
    "print(f\"Loss function:       CrossEntropyLoss (label_smoothing={config['label_smoothing']})\")\n",
    "print(f\"Gradient clipping:   {config['max_grad_norm']}\")\n",
    "print(f\"Weight decay:        {config['weight_decay']}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Start Training\n",
    "\n",
    "**Target: 82%+ validation accuracy in 90 epochs**\n",
    "\n",
    "**Expected Timeline (training from scratch on 1000 classes):**\n",
    "- Epochs 1-30:  Warmup and initial learning (15-35% val acc)\n",
    "- Epochs 31-60: Steady improvement (35-55% val acc)\n",
    "- Epochs 61-90: Continued learning (55-75% val acc)\n",
    "- May need 100-120 epochs for 80%+ accuracy\n",
    "\n",
    "**Hardware Performance:**\n",
    "- **g5.xlarge (A10G 24GB):** ~8-10 min/epoch\n",
    "  - 90 epochs: ~12-15 hours\n",
    "  - Cost: ~$12-15 (on-demand) or ~$4-5 (spot)\n",
    "\n",
    "- **g5.2xlarge (A10G 24GB + more CPU):** ~6-8 min/epoch  \n",
    "  - 90 epochs: ~9-12 hours\n",
    "  - Cost: ~$11-15 (on-demand) or ~$3-5 (spot)\n",
    "\n",
    "- **p3.2xlarge (V100 16GB):** ~5-6 min/epoch\n",
    "  - 90 epochs: ~7-9 hours\n",
    "  - Cost: ~$21-27 (on-demand) or ~$6-8 (spot)\n",
    "\n",
    "**Checkpoints:**\n",
    "- Best model saved automatically\n",
    "- Checkpoints saved every 5 epochs\n",
    "- Training can resume from last checkpoint if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING - ImageNet 1000 Classes\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target epochs:              {config['num_epochs']}\")\n",
    "print(f\"Goal:                       82%+ validation accuracy\")\n",
    "print(f\"Training samples:           ~1.28M images\")\n",
    "print(f\"Validation samples:         ~50K images\")\n",
    "print(f\"Batch size:                 {config['batch_size']}\")\n",
    "print(f\"Iterations per epoch:       {len(train_loader):,}\")\n",
    "print(f\"Total iterations:           {len(train_loader) * config['num_epochs']:,}\")\n",
    "print(f\"Augmentation:               {config['augmentation_strength']}\")\n",
    "print(f\"Device:                     {device}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n⏰ Training will take 7-15 hours depending on GPU.\")\n",
    "print(\"📊 Progress will be displayed after each epoch.\")\n",
    "print(\"💾 Checkpoints saved every 5 epochs + best model.\")\n",
    "print(\"\\n🚀 Starting training now...\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = trainer.train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=config['num_epochs'],\n",
    "    device=device,\n",
    "    save_frequency=config['save_frequency']\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_hours = (end_time - start_time) / 3600\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total training time: {training_time_hours:.2f} hours\")\n",
    "print(f\"Average time per epoch: {training_time_hours / config['num_epochs']:.2f} hours\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Training and Validation Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2, color='blue')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2, color='orange')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training and Validation Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Accuracy', linewidth=2, color='blue')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Accuracy', linewidth=2, color='orange')\n",
    "axes[0, 1].axhline(y=82, color='red', linestyle='--', label='82% Target', alpha=0.7, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning Rate Schedule\n",
    "axes[1, 0].plot(history['learning_rate'], linewidth=2, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 0].set_title('Learning Rate Schedule (OneCycleLR)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation Accuracy Zoomed\n",
    "axes[1, 1].plot(history['val_acc'], label='Val Accuracy', linewidth=3, color='green')\n",
    "axes[1, 1].axhline(y=82, color='red', linestyle='--', label='82% Target', alpha=0.7, linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Validation Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_1000classes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL TRAINING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Accuracy:  {max(history['val_acc']):.2f}%\")\n",
    "print(f\"Final Training Accuracy:   {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Training Loss:       {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss:     {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Train/Val Accuracy Gap:    {history['train_acc'][-1] - history['val_acc'][-1]:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if max(history['val_acc']) >= 82:\n",
    "    print(\"\\n🎉 TARGET ACHIEVED: 82%+ validation accuracy!\")\n",
    "    print(\"✓ Successfully trained ResNet50 on full ImageNet-1K!\")\n",
    "    print(\"✓ Model ready for deployment and inference.\")\n",
    "elif max(history['val_acc']) >= 75:\n",
    "    print(f\"\\n✓ Good progress! Reached {max(history['val_acc']):.2f}% validation accuracy.\")\n",
    "    print(\"\\nTo reach 82%+ accuracy, consider:\")\n",
    "    print(\"  - Train for more epochs (100-120 total)\")\n",
    "    print(\"  - Adjust learning rate (try lower max_lr)\")\n",
    "    print(\"  - Fine-tune from current checkpoint\")\nelse:\n",
    "    print(f\"\\n⚠️  Current best: {max(history['val_acc']):.2f}% (target: 82%+)\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"  - Train for more epochs (need 100-120 for full convergence)\")\n",
    "    print(\"  - Check learning rate (may need adjustment)\")\n",
    "    print(\"  - Verify data augmentation is appropriate\")\n",
    "    print(\"  - Consider starting from pretrained weights\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Evaluate Best Model\n",
    "\n",
    "Load the best checkpoint and evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading best checkpoint for final evaluation...\")\n",
    "\n",
    "# Load best model\n",
    "best_checkpoint_path = Path(config['checkpoint_dir']) / 'best_model.pth'\n",
    "if best_checkpoint_path.exists():\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Best validation accuracy: {checkpoint['best_acc']:.2f}%\")\n",
    "else:\n",
    "    print(\"⚠️  Best checkpoint not found, using current model\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nRunning final evaluation on full validation set (50,000 images)...\")\n",
    "print(\"This will take ~5-10 minutes...\\n\")\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "top5_correct = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Evaluating'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Top-1 accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Top-5 accuracy\n",
    "        _, top5_pred = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "        top5_correct += top5_pred.eq(labels.view(-1, 1).expand_as(top5_pred)).sum().item()\n",
    "\n",
    "top1_acc = 100. * correct / total\n",
    "top5_acc = 100. * top5_correct / total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL VALIDATION RESULTS - ImageNet 1000 Classes\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Top-1 Accuracy:          {top1_acc:.2f}%\")\n",
    "print(f\"Top-5 Accuracy:          {top5_acc:.2f}%\")\n",
    "print(f\"Total samples evaluated: {total:,}\")\n",
    "print(f\"Number of classes:       1000\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if top1_acc >= 82:\n",
    "    print(\"\\n🎉 EXCELLENT! Achieved 82%+ top-1 accuracy!\")\n",
    "    print(\"✓ Model performance exceeds ImageNet competition baseline.\")\n",
    "elif top1_acc >= 76:\n",
    "    print(\"\\n✓ GREAT! Achieved competitive ImageNet accuracy.\")\n",
    "    print(f\"  Top-1: {top1_acc:.2f}% (official ResNet50 baseline: 76.13%)\")\n",
    "    print(f\"  Top-5: {top5_acc:.2f}%\")\n",
    "elif top1_acc >= 70:\n",
    "    print(\"\\n✓ GOOD! Solid performance for training from scratch.\")\n",
    "    print(f\"  Consider training more epochs for further improvement.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Top-1: {top1_acc:.2f}% (target: 82%+)\")\n",
    "    print(\"  Model may benefit from:\")\n",
    "    print(\"  - More training epochs\")\n",
    "    print(\"  - Learning rate adjustment\")\n",
    "    print(\"  - Transfer learning from pretrained weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save Final Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deployment\n",
    "deployment_dir = Path('./deployment')\n",
    "deployment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save complete model with metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': config['num_classes'],\n",
    "    'final_val_acc': top1_acc,\n",
    "    'final_top5_acc': top5_acc,\n",
    "    'config': config,\n",
    "    'training_history': history,\n",
    "    'pytorch_version': torch.__version__,\n",
    "}, deployment_dir / 'resnet50_1000class_final.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL SAVED FOR DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location:            {deployment_dir / 'resnet50_1000class_final.pth'}\")\n",
    "print(f\"Model size:          ~{Path(deployment_dir / 'resnet50_1000class_final.pth').stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"Top-1 Accuracy:      {top1_acc:.2f}%\")\n",
    "print(f\"Top-5 Accuracy:      {top5_acc:.2f}%\")\n",
    "print(f\"Number of classes:   {config['num_classes']}\")\n",
    "print(f\"PyTorch version:     {torch.__version__}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ Model ready for inference and deployment!\")\n",
    "print(\"\\nTo use this model:\")\n",
    "print(\"```python\")\n",
    "print(\"checkpoint = torch.load('deployment/resnet50_1000class_final.pth')\")\n",
    "print(\"model = create_resnet50(num_classes=1000)\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"model.eval()\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Training Complete!\n",
    "\n",
    "You've successfully trained ResNet50 on the full ImageNet-1K dataset (1000 classes, 1.28M images).\n",
    "\n",
    "### Key Achievements:\n",
    "- ✅ Trained from scratch on 1000 classes\n",
    "- ✅ Used LR finder to optimize learning rates\n",
    "- ✅ Applied OneCycleLR scheduler\n",
    "- ✅ Medium augmentation for stable training\n",
    "- ✅ Model checkpoints saved throughout training\n",
    "\n",
    "### Results Summary:\n",
    "- Best validation accuracy achieved\n",
    "- Training time and cost tracked\n",
    "- Final model saved for deployment\n",
    "\n",
    "### If You Want to Improve Further:\n",
    "\n",
    "1. **Train Longer:**\n",
    "   - Continue from last checkpoint for 10-20 more epochs\n",
    "   - Accuracy often improves with extended training\n",
    "\n",
    "2. **Adjust Learning Rate:**\n",
    "   - Try lower max_lr (e.g., current_max_lr / 2)\n",
    "   - Experiment with pct_start (0.4-0.6)\n",
    "\n",
    "3. **Try Transfer Learning:**\n",
    "   - Start with official pretrained weights\n",
    "   - Fine-tune for 10-20 epochs\n",
    "   - Can reach 80-82% much faster\n",
    "\n",
    "4. **Experiment with Augmentation:**\n",
    "   - Try 'light' or 'heavy' augmentation\n",
    "   - Add MixUp or CutMix\n",
    "\n",
    "### Cost Optimization Tips:\n",
    "\n",
    "1. **Use Spot Instances:**\n",
    "   - 60-70% cheaper than on-demand\n",
    "   - g5.xlarge spot: ~$0.30/hour (vs $1.01/hour)\n",
    "\n",
    "2. **Checkpoint Frequently:**\n",
    "   - Save every 5 epochs (already configured)\n",
    "   - Can resume if spot instance interrupted\n",
    "\n",
    "3. **Monitor Training:**\n",
    "   - Stop early if val accuracy plateaus\n",
    "   - No need to waste compute on unhelpful epochs\n",
    "\n",
    "### Download & Cleanup:\n",
    "\n",
    "```bash\n",
    "# Download results to local machine\n",
    "scp -i your-key.pem ubuntu@<instance-ip>:~/Image_net_training_model/deployment/* ./\n",
    "scp -i your-key.pem ubuntu@<instance-ip>:~/Image_net_training_model/*.png ./\n",
    "\n",
    "# After downloading, cleanup AWS:\n",
    "# 1. Terminate EC2 instance\n",
    "# 2. Delete or snapshot EBS volume\n",
    "# 3. Verify no running resources in AWS Console\n",
    "```\n",
    "\n",
    "### Congratulations on completing ImageNet-1K training! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}